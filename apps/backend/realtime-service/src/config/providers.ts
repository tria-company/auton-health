import { AzureOpenAI } from 'openai';
import OpenAI from 'openai';
import { config } from './index';
import { aiPricingService, LLMType, AIStage } from '../services/aiPricingService';

// Configura√ß√£o do Azure OpenAI
export const openaiClient = new AzureOpenAI({
  endpoint: config.AZURE_OPENAI_ENDPOINT,
  apiKey: config.AZURE_OPENAI_API_KEY,
  apiVersion: config.AZURE_OPENAI_CHAT_API_VERSION,
  timeout: 30000, // 30 segundos
  maxRetries: 3,
});

// Teste de conex√£o com Azure OpenAI
export async function testOpenAIConnection(): Promise<boolean> {
  try {
    // Azure OpenAI n√£o tem endpoint models.list(), ent√£o testamos com uma chamada simples
    const response = await openaiClient.chat.completions.create({
      model: config.AZURE_OPENAI_CHAT_DEPLOYMENT,
      messages: [{ role: 'user', content: 'test' }],
      max_tokens: 1,
    });

    if (response.choices && response.choices.length > 0) {
      console.log('‚úÖ Conex√£o com Azure OpenAI estabelecida com sucesso');
      console.log(`   Deployment: ${config.AZURE_OPENAI_CHAT_DEPLOYMENT}`);
      return true;
    }

    console.error('‚ùå Azure OpenAI conectado mas resposta inv√°lida');
    return false;
  } catch (error) {
    console.error('‚ùå Falha ao conectar com Azure OpenAI:', error);
    return false;
  }
}

// LiveKit removido - usando WebRTC direto via WebSocket

// Configura√ß√£o Redis (opcional por enquanto)
export const redisSettings = config.REDIS_URL
  ? {
    url: config.REDIS_URL,
    password: config.REDIS_PASSWORD,
    retryDelayOnFailover: 100,
    maxRetriesPerRequest: 3,
    lazyConnect: true,
  }
  : null;

// Alias export compat√≠vel com redis.ts
export const redisConfig = redisSettings;

// Teste de conex√£o Redis (se configurado)
export async function testRedisConnection(): Promise<boolean> {
  if (!redisSettings) {
    console.log('‚ö†Ô∏è  Redis n√£o configurado - usando modo sem cache');
    return true; // N√£o √© erro, apenas n√£o est√° configurado
  }

  try {
    // TODO: Implementar teste de conex√£o Redis quando necess√°rio
    console.log('‚ö†Ô∏è  Redis configurado mas teste n√£o implementado ainda');
    return true;
  } catch (error) {
    console.error('‚ùå Falha ao conectar com Redis:', error);
    return false;
  }
}

// Configura√ß√µes de AI espec√≠ficas para diferentes modelos
export const aiModels = {
  transcription: {
    openai: {
      model: 'whisper-1',
      language: 'pt',
      response_format: 'verbose_json' as const,
      temperature: 0,
    },
    // Placeholder para outros providers
    google: {
      model: 'latest_long',
      languageCode: 'pt-BR',
      enableAutomaticPunctuation: true,
      useEnhanced: true,
    },
  },

  completion: {
    model: config.AZURE_OPENAI_CHAT_DEPLOYMENT, // Usa deployment name do Azure
    temperature: config.LLM_TEMPERATURE,
    max_tokens: config.LLM_MAX_TOKENS,
    top_p: 1,
    frequency_penalty: 0,
    presence_penalty: 0,
  },

  embedding: {
    model: 'text-embedding-3-small',
    dimensions: 1536,
  },
};

// Helper para criar chat completion
export async function makeChatCompletion(
  messages: OpenAI.Chat.ChatCompletionMessageParam[],
  options: Partial<OpenAI.Chat.ChatCompletionCreateParams> = {},
  trackingOptions?: {
    etapa?: AIStage;
    consultaId?: string;
  }
) {
  const params: OpenAI.Chat.ChatCompletionCreateParams = {
    ...aiModels.completion,
    messages,
    ...options,
    // garantir n√£o-stream para que 'choices' exista
    stream: false,
  };

  const response = await openaiClient.chat.completions.create(params);

  // üìä Registrar uso para monitoramento de custos
  if (response.usage) {
    const model = (params.model || aiModels.completion.model) as LLMType;
    const etapa = trackingOptions?.etapa || 'chat_completion';

    // Extrair cached tokens (se dispon√≠vel na API version preview)
    const cachedTokens = (response.usage as any).prompt_tokens_details?.cached_tokens || 0;

    await aiPricingService.logChatCompletionUsage(
      model,
      response.usage.prompt_tokens,
      response.usage.completion_tokens,
      etapa,
      trackingOptions?.consultaId,
      cachedTokens
    );
  }

  return response;
}

// Helper para criar embeddings
export async function makeEmbedding(
  text: string,
  trackingOptions?: {
    consultaId?: string;
  }
) {
  const response = await openaiClient.embeddings.create({
    model: aiModels.embedding.model,
    input: text,
    dimensions: aiModels.embedding.dimensions,
  });

  // üìä Registrar uso para monitoramento de custos
  if (response.usage) {
    const model = aiModels.embedding.model as 'text-embedding-3-small' | 'text-embedding-3-large';

    await aiPricingService.logEmbeddingUsage(
      model,
      response.usage.total_tokens,
      trackingOptions?.consultaId
    );
  }

  return response;
}

// Valida√ß√£o de todas as configura√ß√µes
export async function validateAllProviders(): Promise<{
  openai: boolean;
  redis: boolean;
}> {
  console.log('üîÑ Validando conex√µes com provedores...\n');

  const results = {
    openai: await testOpenAIConnection(),
    redis: await testRedisConnection(),
  };

  console.log('\nüìä Resultado da valida√ß√£o:');
  console.log(`   OpenAI: ${results.openai ? '‚úÖ' : '‚ùå'}`);
  console.log(`   Redis: ${results.redis ? '‚úÖ' : '‚ö†Ô∏è'}`);

  return results;
}