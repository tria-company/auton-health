import { ProcessedAudioChunk } from './audioProcessor';
import { db, logError, logWarning } from '../config/database';
import { randomUUID } from 'crypto';
import { aiConfig } from '../config';
import FormData from 'form-data';
import { aiPricingService } from './aiPricingService';

export interface TranscriptionResult {
  id: string;
  sessionId: string;
  speaker: 'doctor' | 'patient';
  text: string;
  confidence: number;
  timestamp: string;
  startTime: number;
  endTime: number;
  is_final: boolean;
}

export interface ASRConfig {
  language: string;
  model: string;
  enablePunctuation: boolean;
  enableWordTimestamps: boolean;
}

class ASRService {
  private config: ASRConfig = {
    language: 'pt-BR',
    model: 'whisper-1', // Modelo OpenAI Whisper
    enablePunctuation: true,
    enableWordTimestamps: true // HABILITADO para melhor contexto
  };

  // üéØ CONFIGURA√á√ïES OTIMIZADAS PARA TRANSCRI√á√ÉO M√âDICA
  private whisperConfig = {
    temperature: 0.0, // M√°xima determina√ß√£o (era 0.2)
    language: 'pt', // Portugu√™s brasileiro
    response_format: 'verbose_json' as const,
    // ‚úÖ Prompt melhorado para contexto m√©dico brasileiro e evitar transcri√ß√µes estranhas
    prompt: 'Esta √© uma consulta m√©dica profissional em portugu√™s brasileiro entre m√©dico e paciente. Transcreva APENAS o que foi realmente dito na consulta. Use terminologia m√©dica adequada. N√ÉO invente palavras ou frases. N√ÉO adicione conte√∫do que n√£o foi falado. N√ÉO transcreva ru√≠do ou sil√™ncio como palavras. Palavras comuns: doutor, dor, sintoma, medicamento, tratamento, exame, diagn√≥stico, consulta, paciente, m√©dico.'
  };

  private isEnabled = false;
  private enableSimulation = false; // Flag para controlar simula√ß√£o - PERMANENTEMENTE DESABILITADA

  // Azure OpenAI config
  private azureEndpoint: string = '';
  private azureApiKey: string = '';
  private azureDeployment: string = '';
  private azureApiVersion: string = '';

  constructor() {
    this.checkASRAvailability();
  }

  // Verificar se ASR est√° dispon√≠vel/configurado
  private checkASRAvailability(): void {
    const hasAzure = Boolean(aiConfig.azure.endpoint && aiConfig.azure.apiKey);

    if (hasAzure) {
      try {
        // Configurar Azure OpenAI
        this.azureEndpoint = aiConfig.azure.endpoint;
        this.azureApiKey = aiConfig.azure.apiKey;
        this.azureDeployment = aiConfig.azure.deployments.whisper;
        this.azureApiVersion = aiConfig.azure.apiVersions.whisper;

        this.isEnabled = true;
        console.log('‚úÖ Azure OpenAI Whisper ASR Service habilitado');
      } catch (error) {
        console.error('‚ùå Erro ao inicializar Azure OpenAI:', error);
        logError(
          `Erro ao inicializar Azure OpenAI Whisper ASR`,
          'error',
          null,
          { error: error instanceof Error ? error.message : String(error) }
        );
        this.isEnabled = false;
      }
    } else {
      console.warn('‚ö†Ô∏è ASR Service desabilitado - Configure AZURE_OPENAI_ENDPOINT e AZURE_OPENAI_API_KEY');
      this.isEnabled = false;
    }
  }

  // Processar √°udio e retornar transcri√ß√£o
  public async processAudio(audioChunk: ProcessedAudioChunk): Promise<TranscriptionResult | null> {
    const asrId = `asr_${Date.now()}_${Math.random().toString(36).substr(2, 6)}`;

    // üîç DEBUG: Log de entrada no ASR
    console.log(`üîç DEBUG [ASR] processAudio CHAMADO:`, {
      asrId,
      sessionId: audioChunk.sessionId,
      channel: audioChunk.channel,
      duration: audioChunk.duration,
      hasVoiceActivity: audioChunk.hasVoiceActivity,
      averageVolume: audioChunk.averageVolume,
      isEnabled: this.isEnabled,
      hasOpenAI: !!this.azureApiKey,
      chunkId: audioChunk.sessionId + ':' + audioChunk.channel + ':' + audioChunk.timestamp
    });

    if (!this.isEnabled || !this.azureApiKey) {
      console.warn(`‚ö†Ô∏è [ASR] ASR n√£o est√° habilitado ou Azure OpenAI n√£o configurado - ${asrId}`);
      console.warn(`‚ö†Ô∏è [ASR] N√£o √© poss√≠vel transcrever √°udio sem Azure OpenAI Whisper configurado`);
      // ‚úÖ N√ÉO usar simula√ß√£o ou fallback - retornar null se n√£o houver Whisper
      return null;
    }

    try {
      console.log(`üîç DEBUG [ASR] USANDO WHISPER - ${asrId}`);
      // Usar OpenAI Whisper para transcri√ß√£o real
      const result = await this.transcribeWithWhisper(audioChunk);
      /**
      console.log(`üîç DEBUG [ASR] WHISPER RESULTADO - ${asrId}:`, {
        hasResult: !!result,
        text: result?.text || null,
        id: result?.id || null
      });
       */
      return result;

    } catch (error) {
      console.error(`‚ùå [ASR] ERRO WHISPER - ${asrId}:`, error);
      console.error(`‚ùå [ASR] N√£o √© poss√≠vel transcrever √°udio devido a erro no Whisper`);
      logError(
        `Erro no processamento de √°udio via Whisper ASR`,
        'error',
        audioChunk.sessionId,
        { asrId, channel: audioChunk.channel, duration: audioChunk.duration, error: error instanceof Error ? error.message : String(error) }
      );
      // ‚úÖ N√ÉO usar fallback - retornar null em caso de erro
      // Isso evita transcri√ß√µes incorretas ou simuladas
      return null;
    }
  }

  // Simular transcri√ß√£o para desenvolvimento
  private async simulateTranscription(audioChunk: ProcessedAudioChunk): Promise<TranscriptionResult | null> {
    // Verificar se h√° atividade de voz suficiente no chunk
    if (!audioChunk.hasVoiceActivity) {
      // N√£o gerar transcri√ß√£o para sil√™ncio ou ru√≠do baixo
      return null;
    }

    // Simular delay de processamento realista
    await new Promise(resolve => setTimeout(resolve, 800 + Math.random() * 1200));

    // Gerar texto simulado baseado no speaker e contexto
    const simulatedTexts = {
      doctor: [
        'Como voc√™ est√° se sentindo hoje?',
        'Pode me contar mais sobre os sintomas?',
        'H√° quanto tempo isso est√° acontecendo?',
        'Voc√™ tem alguma alergia a medicamentos?',
        'Vamos examinar os sintomas que voc√™ mencionou.',
        'Baseado no que voc√™ me disse, acredito que seja...',
        'Vou prescrever um medicamento para ajudar.',
        'Preciso que voc√™ tome este medicamento conforme prescrito.',
        'Vamos agendar um retorno em duas semanas.',
        'Tem alguma pergunta sobre o tratamento?',
        'Muito bem, vamos prosseguir.',
        'Entendo a sua preocupa√ß√£o.'
      ],
      patient: [
        'Doutor, estou sentindo uma dor no peito.',
        'A dor come√ßou ontem √† noite.',
        'Est√° doendo mais quando respiro fundo.',
        'Sim, j√° tomei este medicamento antes.',
        'N√£o, n√£o tenho alergia a medicamentos.',
        'Entendi, vou seguir as instru√ß√µes.',
        'Quando devo retornar?',
        'Obrigado, doutor.',
        'Posso fazer exerc√≠cios normalmente?',
        'E se a dor n√£o melhorar?',
        'Estou preocupado com isso.',
        'Muito obrigado pela consulta.'
      ]
    };

    // Calcular probabilidade de gerar transcri√ß√£o baseada na dura√ß√£o e intensidade
    const probabilityFactor = Math.min(audioChunk.duration / 2000, 1); // Normalizar para chunks de 2s
    const intensityFactor = Math.min(audioChunk.averageVolume / 0.1, 1); // Normalizar volume
    const shouldGenerate = Math.random() < (probabilityFactor * intensityFactor * 0.7); // 70% chance max

    if (!shouldGenerate) {
      return null;
    }

    const texts = simulatedTexts[audioChunk.channel];
    const randomText = texts[Math.floor(Math.random() * texts.length)];

    // Ajustar confian√ßa baseada na qualidade do √°udio simulada
    const baseConfidence = 0.75 + (intensityFactor * 0.2); // 75-95% baseado no volume
    const confidence = Math.min(baseConfidence + Math.random() * 0.1, 0.99);

    // Criar resultado de transcri√ß√£o
    const transcriptionResult: TranscriptionResult = {
      id: randomUUID(), // Gerar UUID v√°lido
      sessionId: audioChunk.sessionId,
      speaker: audioChunk.channel,
      text: randomText,
      confidence: Math.round(confidence * 100) / 100,
      timestamp: new Date().toISOString(),
      startTime: Math.round(audioChunk.timestamp - audioChunk.duration),
      endTime: Math.round(audioChunk.timestamp),
      is_final: true
    };

    // Salvar no banco de dados
    try {
      await this.saveTranscription(transcriptionResult);
      console.log(`üìù Transcri√ß√£o simulada: [${audioChunk.channel}] "${randomText}" (conf: ${Math.round(confidence * 100)}%)`);
    } catch (error) {
      console.error('Erro ao salvar transcri√ß√£o:', error);
    }

    return transcriptionResult;
  }

  // Gerar transcri√ß√£o baseada em an√°lise real do √°udio
  private async generateRealBasedTranscription(audioChunk: ProcessedAudioChunk): Promise<TranscriptionResult | null> {
    // Verificar se h√° atividade de voz suficiente no chunk
    if (!audioChunk.hasVoiceActivity) {
      return null;
    }

    // Simular delay de processamento realista
    await new Promise(resolve => setTimeout(resolve, 500 + Math.random() * 800));

    // Analisar caracter√≠sticas do √°udio para gerar texto mais realista
    const intensity = audioChunk.averageVolume;
    const duration = audioChunk.duration;

    // Gerar texto baseado na dura√ß√£o e intensidade do √°udio
    let transcribedText = '';

    if (duration < 1000) {
      // Falas curtas (< 1s) - palavras simples
      const shortPhrases = ['Sim', 'N√£o', 'Certo', 'Entendi', 'Obrigado', 'Por favor', 'Desculpe'];
      transcribedText = shortPhrases[Math.floor(Math.random() * shortPhrases.length)];
    } else if (duration < 3000) {
      // Falas m√©dias (1-3s) - frases curtas
      const mediumPhrases = [
        'Est√° bem, doutor',
        'Muito obrigado',
        'N√£o tenho certeza',
        'Pode repetir?',
        'Estou entendendo',
        'Vou fazer isso',
        'Preciso pensar'
      ];
      transcribedText = mediumPhrases[Math.floor(Math.random() * mediumPhrases.length)];
    } else {
      // Falas longas (> 3s) - frases complexas
      const longPhrases = [
        'Estou sentindo uma dor aqui do lado direito',
        'Doutor, preciso falar sobre os medicamentos',
        'N√£o estou me sentindo muito bem ultimamente',
        'Gostaria de saber sobre os resultados dos exames',
        'Tem alguma coisa que posso fazer para melhorar?',
        'Essa dor come√ßou h√° alguns dias e n√£o passa'
      ];
      transcribedText = longPhrases[Math.floor(Math.random() * longPhrases.length)];
    }

    // Adicionar indicador de intensidade
    if (intensity > 0.1) {
      transcribedText += ' [voz alta]';
    } else if (intensity < 0.05) {
      transcribedText += ' [voz baixa]';
    }

    // Ajustar confian√ßa baseada na dura√ß√£o e intensidade
    const durationFactor = Math.min(duration / 2000, 1); // Normalizar para 2s
    const intensityFactor = Math.min(intensity / 0.1, 1); // Normalizar para 0.1
    const confidence = 0.6 + (durationFactor * 0.2) + (intensityFactor * 0.2);

    // Criar resultado de transcri√ß√£o
    const transcriptionResult: TranscriptionResult = {
      id: randomUUID(),
      sessionId: audioChunk.sessionId,
      speaker: audioChunk.channel,
      text: transcribedText,
      confidence: Math.min(confidence, 0.95),
      timestamp: new Date().toISOString(),
      startTime: Math.round(audioChunk.timestamp - audioChunk.duration),
      endTime: Math.round(audioChunk.timestamp),
      is_final: true
    };

    // Salvar no banco de dados
    try {
      await this.saveTranscription(transcriptionResult);
      console.log(`üéØ Transcri√ß√£o baseada em an√°lise real: [${audioChunk.channel}] "${transcribedText}" (${duration}ms, vol: ${intensity.toFixed(3)}, conf: ${Math.round(confidence * 100)}%)`);

      // Trigger gera√ß√£o de sugest√µes ap√≥s salvar transcri√ß√£o
      await this.triggerSuggestionGeneration(transcriptionResult);
    } catch (error) {
      console.error('Erro ao salvar transcri√ß√£o:', error);
    }

    return transcriptionResult;
  }

  // Salvar transcri√ß√£o no banco de dados (usando array √∫nico)
  private async saveTranscription(transcription: TranscriptionResult): Promise<void> {
    try {
      // ‚úÖ Validar sessionId antes de salvar
      if (!transcription.sessionId) {
        console.error('‚ùå [SAVE] sessionId n√£o fornecido, n√£o √© poss√≠vel salvar transcri√ß√£o');
        console.error('‚ùå [SAVE] TranscriptionResult completo:', JSON.stringify(transcription, null, 2));
        logWarning(
          `sessionId n√£o fornecido ao salvar transcri√ß√£o - transcri√ß√£o perdida`,
          null,
          { speaker: transcription.speaker, textLength: transcription.text?.length || 0 }
        );
        return;
      }

      // ‚úÖ Garantir que speaker est√° no formato correto
      let speaker: 'doctor' | 'patient' | 'system' = 'system';
      const speakerLower = (transcription.speaker || '').toLowerCase();
      if (speakerLower.includes('doctor') || speakerLower.includes('m√©dico') || speakerLower.includes('medico') || speakerLower.includes('host')) {
        speaker = 'doctor';
      } else if (speakerLower.includes('patient') || speakerLower.includes('paciente') || speakerLower.includes('participant')) {
        speaker = 'patient';
      }

      // ‚úÖ Usar addTranscriptionToSession para salvar em array √∫nico
      // Para consultas presenciais, usar o speaker como speaker_id (j√° que n√£o temos nome real aqui)
      const speakerId = speaker; // Em consultas presenciais, pode n√£o ter o nome real dispon√≠vel

      const success = await db.addTranscriptionToSession(transcription.sessionId, {
        speaker: speaker,
        speaker_id: speakerId,
        text: transcription.text,
        confidence: transcription.confidence,
        start_ms: transcription.startTime,
        end_ms: transcription.endTime
      });

      if (!success) {
        console.warn('‚ö†Ô∏è [SAVE] Falha ao adicionar transcri√ß√£o ao array');
      } else {
        console.log(`‚úÖ [SAVE] Transcri√ß√£o adicionada ao array: [${speaker}] "${transcription.text.substring(0, 30)}..."`);
      }
    } catch (error) {
      console.error('‚ùå [SAVE] Erro ao salvar transcri√ß√£o no banco:', error);
      if (error instanceof Error) {
        console.error('‚ùå [SAVE] Stack trace:', error.stack);
      }
      logError(
        `Erro ao salvar transcri√ß√£o no banco de dados`,
        'error',
        transcription.sessionId,
        { speaker: transcription.speaker, textLength: transcription.text?.length || 0, error: error instanceof Error ? error.message : String(error) }
      );
      // N√£o lan√ßar erro para n√£o bloquear o fluxo de transcri√ß√£o
    }
  }

  // Processar resposta do Whisper
  private async processWhisperResponse(result: any, audioChunk: ProcessedAudioChunk): Promise<TranscriptionResult | null> {
    // Verificar se h√° texto transcrito
    if (!result.text || result.text.trim().length === 0) {
      console.log(`üîá Whisper n√£o detectou fala clara: ${audioChunk.channel}`);
      return null;
    }

    // üîß P√ìS-PROCESSAMENTO do texto para melhorar qualidade
    const rawText = result.text.trim();

    // ‚úÖ Filtrar textos inv√°lidos ANTES de processar
    if (!this.filterInvalidTranscriptions(rawText)) {
      console.log(`üîá [ASR] Texto inv√°lido descartado: "${rawText}"`);
      return null;
    }

    const cleanedText = this.postProcessTranscription(rawText);

    // Verificar se texto limpo n√£o ficou vazio
    if (!cleanedText || cleanedText.length < 2) {
      console.log(`üîá Texto muito curto ap√≥s limpeza: "${cleanedText}"`);
      return null;
    }

    // ‚úÖ Valida√ß√£o adicional: verificar se o texto faz sentido para consulta m√©dica
    if (!this.filterInvalidTranscriptions(cleanedText)) {
      console.log(`üîá [ASR] Texto limpo ainda inv√°lido, descartando: "${cleanedText}"`);
      return null;
    }

    // ‚úÖ Mapear speaker para valores aceitos pelo schema ('doctor', 'patient')
    let speaker: 'doctor' | 'patient' = 'patient'; // Default para patient
    const channelLower = audioChunk.channel?.toLowerCase() || '';
    if (channelLower.includes('doctor') || channelLower.includes('m√©dico') || channelLower.includes('medico') || channelLower.includes('host')) {
      speaker = 'doctor';
    } else if (channelLower.includes('patient') || channelLower.includes('paciente') || channelLower.includes('participant')) {
      speaker = 'patient';
    }

    // Criar resultado de transcri√ß√£o
    const transcriptionResult: TranscriptionResult = {
      id: randomUUID(),
      sessionId: audioChunk.sessionId,
      speaker: speaker, // ‚úÖ Usar valor mapeado (sempre 'doctor' ou 'patient')
      text: cleanedText,
      confidence: this.calculateWhisperConfidence(result),
      timestamp: new Date().toISOString(),
      startTime: Math.round(audioChunk.timestamp - audioChunk.duration),
      endTime: Math.round(audioChunk.timestamp),
      is_final: true
    };

    // ‚úÖ Salvar no banco de dados automaticamente
    console.log(`üíæ [AUTO-SAVE] Tentando salvar transcri√ß√£o:`, {
      sessionId: transcriptionResult.sessionId,
      speaker: speaker,
      textLength: cleanedText.length,
      textPreview: cleanedText.substring(0, 50) + '...'
    });

    try {
      await this.saveTranscription(transcriptionResult);
      console.log(`‚úÖ [AUTO-SAVE] Transcri√ß√£o salva automaticamente no banco: ${speaker} - "${cleanedText.substring(0, 30)}..."`);
    } catch (saveError) {
      console.error('‚ùå [AUTO-SAVE] Erro ao salvar transcri√ß√£o automaticamente:', saveError);
      if (saveError instanceof Error) {
        console.error('‚ùå [AUTO-SAVE] Stack:', saveError.stack);
      }
      logError(
        `Erro no auto-save de transcri√ß√£o Whisper`,
        'error',
        transcriptionResult.sessionId,
        { speaker, textLength: cleanedText?.length || 0, error: saveError instanceof Error ? saveError.message : String(saveError) }
      );
      // N√£o bloquear o fluxo se o salvamento falhar
    }

    // üéØ LOG DETALHADO DA TRANSCRI√á√ÉO
    console.log(`üéØ Whisper transcreveu: [${audioChunk.channel}] "${result.text.trim()}" (conf: ${Math.round(transcriptionResult.confidence * 100)}%)`);
    console.log(`üìù [${audioChunk.channel}] [Transcri√ß√£o]: ${cleanedText}`);

    return transcriptionResult;
  }

  // Integra√ß√£o com Azure OpenAI Whisper
  private async transcribeWithWhisper(audioChunk: ProcessedAudioChunk): Promise<TranscriptionResult | null> {
    if (!this.azureApiKey || !audioChunk.hasVoiceActivity) {
      return null;
    }

    try {
      // üîç VALIDA√á√ïES PR√â-ENVIO para evitar erros
      const maxFileSize = 25 * 1024 * 1024; // 25MB limite do Whisper
      const maxDuration = 25 * 60 * 1000; // 25 minutos limite do Whisper

      // Verificar tamanho do buffer
      if (audioChunk.audioBuffer.length > maxFileSize) {
        console.warn(`‚ö†Ô∏è Arquivo muito grande para Whisper: ${audioChunk.audioBuffer.length} bytes (m√°x: ${maxFileSize} bytes)`);
        return null;
      }

      // Verificar dura√ß√£o
      if (audioChunk.duration > maxDuration) {
        console.warn(`‚ö†Ô∏è √Åudio muito longo para Whisper: ${audioChunk.duration}ms (m√°x: ${maxDuration}ms)`);
        return null;
      }

      // Verificar se buffer n√£o est√° vazio
      if (audioChunk.audioBuffer.length === 0) {
        console.warn(`‚ö†Ô∏è Buffer de √°udio vazio para ${audioChunk.channel}`);
        return null;
      }

      // üîç VALIDA√á√ÉO DETALHADA DO BUFFER WAV
      const wavValidation = this.validateWavBuffer(audioChunk.audioBuffer);
      if (!wavValidation.isValid) {
        console.error(`‚ùå Buffer WAV inv√°lido para ${audioChunk.channel}:`, wavValidation.errors);
        logWarning(
          `Buffer WAV inv√°lido detectado - √°udio n√£o processado`,
          audioChunk.sessionId,
          { channel: audioChunk.channel, errors: wavValidation.errors, bufferSize: audioChunk.audioBuffer.length }
        );
        return null;
      }
      console.log(`‚úÖ Buffer WAV v√°lido para ${audioChunk.channel}:`, wavValidation.info);

      // üîç VALIDA√á√ÉO ADICIONAL: Verificar se o buffer n√£o est√° corrompido
      if (audioChunk.audioBuffer.length < 1000) {
        console.warn(`‚ö†Ô∏è Buffer WAV muito pequeno: ${audioChunk.audioBuffer.length} bytes`);
        return null;
      }

      // üîç VALIDA√á√ÉO ADICIONAL: Verificar assinatura RIFF
      const riffSignature = audioChunk.audioBuffer.toString('ascii', 0, 4);
      if (riffSignature !== 'RIFF') {
        console.error(`‚ùå Assinatura RIFF inv√°lida: "${riffSignature}"`);
        return null;
      }

      // üîß CORRE√á√ÉO: Usar FormData de forma mais robusta
      const formData = new FormData();

      // üîß CORRE√á√ÉO: Adicionar model PRIMEIRO (algumas APIs s√£o sens√≠veis √† ordem)
      formData.append('model', this.config.model);

      // Adicionar arquivo com configura√ß√µes espec√≠ficas para Whisper
      formData.append('file', audioChunk.audioBuffer, {
        filename: 'audio.wav',
        contentType: 'audio/wav',
        knownLength: audioChunk.audioBuffer.length
      });

      // Adicionar outros par√¢metros de configura√ß√£o
      formData.append('language', this.whisperConfig.language);
      formData.append('response_format', this.whisperConfig.response_format);
      formData.append('temperature', this.whisperConfig.temperature.toString());
      formData.append('prompt', this.whisperConfig.prompt);

      console.log(`üé§ Enviando √°udio para Whisper: ${audioChunk.channel} - ${audioChunk.duration}ms`);
      //console.log(`üîç DEBUG [AUDIO] Buffer size: ${audioChunk.audioBuffer.length} bytes`);
      //console.log(`üîç DEBUG [AUDIO] Sample rate: ${audioChunk.sampleRate} Hz`);
      //console.log(`üîç DEBUG [AUDIO] Has voice activity: ${audioChunk.hasVoiceActivity}`);
      //console.log(`üîç DEBUG [AUDIO] Average volume: ${audioChunk.averageVolume}`);
      //console.log(`üîç DEBUG [AUDIO] Duration: ${audioChunk.duration}ms`);

      // üîç DEBUG: Verificar par√¢metros do FormData
      //console.log(`üîç DEBUG [FORMDATA] Model: "${this.config.model}"`);
      //console.log(`üîç DEBUG [FORMDATA] Language: "${this.whisperConfig.language}"`);
      //console.log(`üîç DEBUG [FORMDATA] Response format: "${this.whisperConfig.response_format}"`);
      //console.log(`üîç DEBUG [FORMDATA] Temperature: "${this.whisperConfig.temperature}"`);

      // üîß CORRE√á√ÉO: Usar headers corretos e timeout
      console.log(`üöÄ CHAMANDO WHISPER API...`);

      // üîß CORRE√á√ÉO: Usar node-fetch para melhor compatibilidade com FormData
      const fetch = (await import('node-fetch')).default;

      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), 30000); // 30s timeout

      try {
        // Azure OpenAI Whisper endpoint
        const azureUrl = `${this.azureEndpoint}/openai/deployments/${this.azureDeployment}/audio/transcriptions?api-version=${this.azureApiVersion}`;

        console.log(`üåê [ASR] Enviando para Azure: ${azureUrl}`);

        const response = await fetch(azureUrl, {
          method: 'POST',
          headers: {
            'api-key': this.azureApiKey,
            ...formData.getHeaders() // Usar headers do FormData explicitamente
          },
          body: formData,
          signal: controller.signal
        });

        clearTimeout(timeoutId);

        if (!response.ok) {
          const errorText = await response.text();
          throw new Error(`HTTP ${response.status}: ${errorText}`);
        }

        const result = await response.json() as any;

        console.log(`‚úÖ WHISPER API RESPONDEU!`);
        console.log(`üîç DEBUG [WHISPER] Response received:`, result);

        // üìä Registrar uso do Whisper para monitoramento de custos
        await aiPricingService.logWhisperUsage(
          audioChunk.duration,
          audioChunk.sessionId
        );

        return this.processWhisperResponse(result, audioChunk);

      } catch (fetchError: any) {
        clearTimeout(timeoutId);

        if (fetchError.name === 'AbortError') {
          throw new Error('Timeout na API Whisper (30s)');
        }
        throw fetchError;
      }

    } catch (error: any) {
      console.error('Erro na API Whisper:', error);

      // üîç LOG DETALHADO do erro para diagn√≥stico
      console.error(`üîç DEBUG [WHISPER_ERROR] Canal: ${audioChunk.channel}`);
      console.error(`üîç DEBUG [WHISPER_ERROR] Buffer size: ${audioChunk.audioBuffer.length} bytes`);
      console.error(`üîç DEBUG [WHISPER_ERROR] Duration: ${audioChunk.duration}ms`);
      console.error(`üîç DEBUG [WHISPER_ERROR] Sample rate: ${audioChunk.sampleRate} Hz`);
      console.error(`üîç DEBUG [WHISPER_ERROR] Error code: ${error.code || 'N/A'}`);
      console.error(`üîç DEBUG [WHISPER_ERROR] Error status: ${error.status || 'N/A'}`);
      console.error(`üîç DEBUG [WHISPER_ERROR] Error message: ${error.message || 'N/A'}`);
      console.error(`üîç DEBUG [WHISPER_ERROR] Error type: ${error.type || 'N/A'}`);

      logError(
        `Erro na API Whisper - transcri√ß√£o n√£o realizada`,
        'error',
        audioChunk.sessionId,
        {
          channel: audioChunk.channel,
          bufferSize: audioChunk.audioBuffer.length,
          duration: audioChunk.duration,
          errorCode: error.code || 'N/A',
          errorStatus: error.status || 'N/A',
          errorMessage: error.message || 'Erro desconhecido'
        }
      );

      // Se for erro de rede ou API, lan√ßar para usar fallback
      if (error.code === 'ENOTFOUND' || error.status >= 500) {
        throw error;
      }

      // Se for erro de √°udio inv√°lido, retornar null
      console.warn(`‚ö†Ô∏è √Åudio inv√°lido para Whisper: ${audioChunk.channel} - ${error.message || 'Erro desconhecido'}`);
      return null;
    }
  }

  // üîß P√ìS-PROCESSAMENTO da transcri√ß√£o para melhorar qualidade
  private postProcessTranscription(text: string): string {
    if (!text) return text;

    let processed = text;

    // Remover caracteres estranhos e ru√≠dos comuns
    processed = processed.replace(/\[.*?\]/g, ''); // Remove [m√∫sica], [ru√≠do], etc.
    processed = processed.replace(/\(.*?\)/g, ''); // Remove (tosse), (suspiro), etc.
    processed = processed.replace(/[‚ô™‚ô´üéµüé∂]/g, ''); // Remove s√≠mbolos musicais

    // Corrigir espa√ßamento
    processed = processed.replace(/\s+/g, ' '); // M√∫ltiplos espa√ßos ‚Üí espa√ßo √∫nico
    processed = processed.trim();

    // Capitalizar primeira letra se n√£o estiver
    if (processed.length > 0) {
      processed = processed.charAt(0).toUpperCase() + processed.slice(1);
    }

    // Corrigir pontua√ß√£o comum
    processed = processed.replace(/([.!?])\s*([a-z])/g, (match, p1, p2) => {
      return p1 + ' ' + p2.toUpperCase();
    });

    // Garantir ponto final se necess√°rio
    if (processed.length > 3 && !/[.!?]$/.test(processed)) {
      processed += '.';
    }

    console.log(`üîß Texto processado: "${text}" ‚Üí "${processed}"`);
    return processed;
  }

  // ‚úÖ Filtrar textos estranhos que n√£o fazem sentido em consultas m√©dicas
  private filterInvalidTranscriptions(text: string): boolean {
    const invalidPatterns = [
      /se inscreva/i,
      /inscreva-se/i,
      /se inscrevam/i,
      /no nosso canal/i,
      /no canal/i,
      /curtam o v√≠deo/i,
      /deixe seu like/i,
      /compartilhe/i,
      /subscribe/i,
      /youtube/i,
      /canal do youtube/i,
      /tchau.*tchau/i,
      /oi.*tudo bem.*tchau/i
    ];

    for (const pattern of invalidPatterns) {
      if (pattern.test(text)) {
        console.warn(`‚ö†Ô∏è [ASR] Texto inv√°lido filtrado: "${text}" (padr√£o: ${pattern})`);
        return false; // Texto inv√°lido
      }
    }

    return true; // Texto v√°lido
  }

  // Calcular confian√ßa baseada na resposta do Whisper
  private calculateWhisperConfidence(response: any): number {
    // Se tem informa√ß√µes de segmentos, calcular m√©dia
    if (response.segments && response.segments.length > 0) {
      const avgConfidence = response.segments.reduce((sum: number, segment: any) => {
        return sum + (segment.avg_logprob || -0.5);
      }, 0) / response.segments.length;

      // Converter logprob para probabilidade (aproximada)
      const baseConfidence = Math.max(0.3, Math.min(0.95, Math.exp(avgConfidence)));

      // Ajustar confian√ßa baseada na qualidade do texto
      const textQuality = this.assessTextQuality(response.text);
      return Math.min(0.95, baseConfidence * textQuality);
    }

    // Baseado no tamanho e qualidade do texto
    const textLength = response.text.trim().length;
    const textQuality = this.assessTextQuality(response.text);

    let baseConfidence = 0.6;
    if (textLength > 50) baseConfidence = 0.9;
    else if (textLength > 20) baseConfidence = 0.8;
    else if (textLength > 10) baseConfidence = 0.7;

    return Math.min(0.95, baseConfidence * textQuality);
  }

  // Avaliar qualidade do texto transcrito
  private assessTextQuality(text: string): number {
    if (!text) return 0.1;

    let quality = 1.0;

    // Penalizar texto muito repetitivo
    const words = text.toLowerCase().split(/\s+/);
    const uniqueWords = new Set(words);
    const repetitionRatio = uniqueWords.size / words.length;
    if (repetitionRatio < 0.5) quality *= 0.7; // Muito repetitivo

    // Penalizar se s√≥ tem uma palavra repetida
    if (uniqueWords.size === 1 && words.length > 2) quality *= 0.3;

    // Bonificar se tem pontua√ß√£o apropriada
    if (/[.!?]/.test(text)) quality *= 1.1;

    // Penalizar ru√≠dos √≥bvios
    if (/\b(ah|eh|uh|hm|hmm)\b/gi.test(text)) quality *= 0.8;

    return Math.max(0.1, Math.min(1.0, quality));
  }

  // Integra√ß√£o com Google Speech-to-Text (futuro)
  private async transcribeWithGoogleSpeech(audioBuffer: Buffer): Promise<string> {
    // TODO: Implementar integra√ß√£o com Google Speech-to-Text API
    throw new Error('Google Speech integration not implemented yet');
  }

  // Configurar idioma
  public setLanguage(language: string): void {
    this.config.language = language;
    console.log(`üåç ASR language configurado: ${language}`);
  }

  // Configurar modelo
  public setModel(model: string): void {
    this.config.model = model;
    console.log(`ü§ñ ASR model configurado: ${model}`);
  }

  // Obter configura√ß√£o atual
  public getConfig(): ASRConfig {
    return { ...this.config };
  }

  // Obter status do servi√ßo
  public getStatus(): object {
    return {
      enabled: this.isEnabled,
      config: this.config,
      availableModels: ['whisper', 'google-speech'],
      supportedLanguages: ['pt-BR', 'en-US', 'es-ES']
    };
  }

  // Habilitar/desabilitar servi√ßo
  public setEnabled(enabled: boolean): void {
    this.isEnabled = enabled;
    console.log(`üé§ ASR Service ${enabled ? 'habilitado' : 'desabilitado'}`);
  }

  // Habilitar/desabilitar simula√ß√£o (para desenvolvimento)
  // ‚úÖ DESABILITADO PERMANENTEMENTE - n√£o usar simula√ß√£o em produ√ß√£o
  public setSimulationEnabled(enabled: boolean): void {
    // ‚úÖ FOR√áAR sempre desabilitado
    this.enableSimulation = false;
    if (enabled) {
      console.warn(`‚ö†Ô∏è [ASR] Tentativa de habilitar simula√ß√£o foi bloqueada - simula√ß√£o est√° permanentemente desabilitada`);
    }
    console.log(`üîá [ASR] Simula√ß√£o est√° permanentemente desabilitada`);
  }

  // Verificar se simula√ß√£o est√° habilitada
  public isSimulationEnabled(): boolean {
    return this.enableSimulation;
  }

  /**
   * Trigger gera√ß√£o de sugest√µes ap√≥s nova transcri√ß√£o
   * TODO: Migrado para ai-service - implementar chamada HTTP
   */
  private async triggerSuggestionGeneration(transcription: TranscriptionResult): Promise<void> {
    // TODO: A funcionalidade de sugest√µes foi migrada para o microservi√ßo ai-service.
    // Implementar chamada HTTP para: POST ${AI_SERVICE_URL}/api/suggestions
    console.log(`ü§ñ [DISABLED] Gera√ß√£o de sugest√µes desabilitada (migra√ß√£o para ai-service) - sess√£o ${transcription.sessionId}`);
  }

  /**
   * Notifica sugest√µes via WebSocket
   */
  private async notifyWebSocketSuggestions(sessionId: string, suggestions: any[]): Promise<void> {
    try {
      // Tentar obter notifier do WebSocket
      const { SessionNotifier } = await import('../websocket/index');

      // Esta √© uma implementa√ß√£o simplificada - em produ√ß√£o, voc√™ teria uma refer√™ncia global ao notifier
      console.log(`üì° WebSocket notification preparada para sess√£o ${sessionId}: ${suggestions.length} sugest√µes`);

      // TODO: Implementar notifica√ß√£o real via WebSocket
      // Por enquanto, apenas log para debug

    } catch (error) {
      console.log('üì° WebSocket notifier n√£o dispon√≠vel - sugest√µes salvas no banco');
    }
  }

  /**
   * Calcula dura√ß√£o da sess√£o em minutos
   */
  private calculateSessionDuration(startTime: string): number {
    const start = new Date(startTime);
    const now = new Date();
    return Math.floor((now.getTime() - start.getTime()) / (1000 * 60));
  }
  /**
   * Valida se o Buffer WAV est√° no formato correto
   */
  private validateWavBuffer(buffer: Buffer): { isValid: boolean; errors: string[]; info: any } {
    const errors: string[] = [];
    const info: any = {};

    try {
      // Verificar tamanho m√≠nimo (44 bytes = header WAV)
      if (buffer.length < 44) {
        errors.push(`Buffer muito pequeno: ${buffer.length} bytes (m√≠nimo: 44 bytes)`);
        return { isValid: false, errors, info };
      }

      // Verificar RIFF signature
      const riffSignature = buffer.toString('ascii', 0, 4);
      if (riffSignature !== 'RIFF') {
        errors.push(`RIFF signature inv√°lida: "${riffSignature}" (esperado: "RIFF")`);
      }

      // Verificar WAVE format
      const waveFormat = buffer.toString('ascii', 8, 12);
      if (waveFormat !== 'WAVE') {
        errors.push(`WAVE format inv√°lido: "${waveFormat}" (esperado: "WAVE")`);
      }

      // Verificar fmt chunk
      const fmtChunk = buffer.toString('ascii', 12, 16);
      if (fmtChunk !== 'fmt ') {
        errors.push(`fmt chunk inv√°lido: "${fmtChunk}" (esperado: "fmt ")`);
      }

      // Verificar audio format (deve ser 1 = PCM)
      const audioFormat = buffer.readUInt16LE(20);
      if (audioFormat !== 1) {
        errors.push(`Audio format inv√°lido: ${audioFormat} (esperado: 1 para PCM)`);
      }

      // Verificar n√∫mero de canais
      const numChannels = buffer.readUInt16LE(22);
      if (numChannels !== 1) {
        errors.push(`N√∫mero de canais inv√°lido: ${numChannels} (esperado: 1 para mono)`);
      }

      // Verificar sample rate
      const sampleRate = buffer.readUInt32LE(24);
      if (sampleRate !== 44100 && sampleRate !== 48000 && sampleRate !== 16000) {
        errors.push(`Sample rate inv√°lido: ${sampleRate} Hz (esperado: 44100, 48000 ou 16000)`);
      }

      // Verificar bits per sample
      const bitsPerSample = buffer.readUInt16LE(34);
      if (bitsPerSample !== 16) {
        errors.push(`Bits per sample inv√°lido: ${bitsPerSample} (esperado: 16)`);
      }

      // Verificar data chunk
      const dataChunk = buffer.toString('ascii', 36, 40);
      if (dataChunk !== 'data') {
        errors.push(`data chunk inv√°lido: "${dataChunk}" (esperado: "data")`);
      }

      // Verificar se h√° dados de √°udio
      const dataSize = buffer.readUInt32LE(40);
      const expectedDataSize = buffer.length - 44;
      if (dataSize !== expectedDataSize) {
        errors.push(`Tamanho dos dados incorreto: ${dataSize} bytes (esperado: ${expectedDataSize} bytes)`);
      }

      // Verificar se h√° dados de √°udio suficientes
      if (dataSize < 1000) { // M√≠nimo 1KB de √°udio
        errors.push(`Dados de √°udio insuficientes: ${dataSize} bytes (m√≠nimo: 1000 bytes)`);
      }

      // Informa√ß√µes do arquivo
      info.size = buffer.length;
      info.sampleRate = sampleRate;
      info.channels = numChannels;
      info.bitsPerSample = bitsPerSample;
      info.audioFormat = audioFormat;
      info.dataSize = dataSize;
      info.duration = (dataSize / (sampleRate * numChannels * (bitsPerSample / 8))) * 1000; // em ms

      // Verificar se h√° dados de √°udio v√°lidos (n√£o todos zeros)
      const audioData = buffer.slice(44);
      let hasNonZeroData = false;
      for (let i = 0; i < Math.min(audioData.length, 1000); i++) {
        if (audioData[i] !== 0) {
          hasNonZeroData = true;
          break;
        }
      }
      if (!hasNonZeroData) {
        errors.push(`Dados de √°udio parecem estar vazios (todos zeros)`);
      }

      console.log(`üîç Valida√ß√£o WAV: ${buffer.length} bytes, ${sampleRate}Hz, ${numChannels} canal, ${bitsPerSample} bits, ${info.duration.toFixed(0)}ms`);

      return {
        isValid: errors.length === 0,
        errors,
        info
      };

    } catch (error) {
      errors.push(`Erro ao validar WAV: ${error}`);
      return { isValid: false, errors, info };
    }
  }
}

// Inst√¢ncia singleton do servi√ßo ASR
export const asrService = new ASRService();
